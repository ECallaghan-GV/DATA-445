{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e308d9ff",
   "metadata": {},
   "source": [
    "# Exam 1 Take-Home Portion\n",
    "\n",
    "# Evan Callaghan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49832c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce959d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Private</th>\n",
       "      <th>Apps</th>\n",
       "      <th>Accept</th>\n",
       "      <th>Enroll</th>\n",
       "      <th>Top10perc</th>\n",
       "      <th>Top25perc</th>\n",
       "      <th>F.Undergrad</th>\n",
       "      <th>P.Undergrad</th>\n",
       "      <th>Outstate</th>\n",
       "      <th>Room.Board</th>\n",
       "      <th>Books</th>\n",
       "      <th>Personal</th>\n",
       "      <th>PhD</th>\n",
       "      <th>Terminal</th>\n",
       "      <th>S.F.Ratio</th>\n",
       "      <th>perc.alumni</th>\n",
       "      <th>Expend</th>\n",
       "      <th>Grad.Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abilene Christian University</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1660</td>\n",
       "      <td>1232</td>\n",
       "      <td>721</td>\n",
       "      <td>23</td>\n",
       "      <td>52</td>\n",
       "      <td>2885</td>\n",
       "      <td>537</td>\n",
       "      <td>7440</td>\n",
       "      <td>3300</td>\n",
       "      <td>450</td>\n",
       "      <td>2200</td>\n",
       "      <td>70</td>\n",
       "      <td>78</td>\n",
       "      <td>18.1</td>\n",
       "      <td>12</td>\n",
       "      <td>7041</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adelphi University</th>\n",
       "      <td>Yes</td>\n",
       "      <td>2186</td>\n",
       "      <td>1924</td>\n",
       "      <td>512</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>2683</td>\n",
       "      <td>1227</td>\n",
       "      <td>12280</td>\n",
       "      <td>6450</td>\n",
       "      <td>750</td>\n",
       "      <td>1500</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>12.2</td>\n",
       "      <td>16</td>\n",
       "      <td>10527</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adrian College</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1428</td>\n",
       "      <td>1097</td>\n",
       "      <td>336</td>\n",
       "      <td>22</td>\n",
       "      <td>50</td>\n",
       "      <td>1036</td>\n",
       "      <td>99</td>\n",
       "      <td>11250</td>\n",
       "      <td>3750</td>\n",
       "      <td>400</td>\n",
       "      <td>1165</td>\n",
       "      <td>53</td>\n",
       "      <td>66</td>\n",
       "      <td>12.9</td>\n",
       "      <td>30</td>\n",
       "      <td>8735</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agnes Scott College</th>\n",
       "      <td>Yes</td>\n",
       "      <td>417</td>\n",
       "      <td>349</td>\n",
       "      <td>137</td>\n",
       "      <td>60</td>\n",
       "      <td>89</td>\n",
       "      <td>510</td>\n",
       "      <td>63</td>\n",
       "      <td>12960</td>\n",
       "      <td>5450</td>\n",
       "      <td>450</td>\n",
       "      <td>875</td>\n",
       "      <td>92</td>\n",
       "      <td>97</td>\n",
       "      <td>7.7</td>\n",
       "      <td>37</td>\n",
       "      <td>19016</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alaska Pacific University</th>\n",
       "      <td>Yes</td>\n",
       "      <td>193</td>\n",
       "      <td>146</td>\n",
       "      <td>55</td>\n",
       "      <td>16</td>\n",
       "      <td>44</td>\n",
       "      <td>249</td>\n",
       "      <td>869</td>\n",
       "      <td>7560</td>\n",
       "      <td>4120</td>\n",
       "      <td>800</td>\n",
       "      <td>1500</td>\n",
       "      <td>76</td>\n",
       "      <td>72</td>\n",
       "      <td>11.9</td>\n",
       "      <td>2</td>\n",
       "      <td>10922</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Private  Apps  Accept  Enroll  Top10perc  \\\n",
       "Abilene Christian University     Yes  1660    1232     721         23   \n",
       "Adelphi University               Yes  2186    1924     512         16   \n",
       "Adrian College                   Yes  1428    1097     336         22   \n",
       "Agnes Scott College              Yes   417     349     137         60   \n",
       "Alaska Pacific University        Yes   193     146      55         16   \n",
       "\n",
       "                              Top25perc  F.Undergrad  P.Undergrad  Outstate  \\\n",
       "Abilene Christian University         52         2885          537      7440   \n",
       "Adelphi University                   29         2683         1227     12280   \n",
       "Adrian College                       50         1036           99     11250   \n",
       "Agnes Scott College                  89          510           63     12960   \n",
       "Alaska Pacific University            44          249          869      7560   \n",
       "\n",
       "                              Room.Board  Books  Personal  PhD  Terminal  \\\n",
       "Abilene Christian University        3300    450      2200   70        78   \n",
       "Adelphi University                  6450    750      1500   29        30   \n",
       "Adrian College                      3750    400      1165   53        66   \n",
       "Agnes Scott College                 5450    450       875   92        97   \n",
       "Alaska Pacific University           4120    800      1500   76        72   \n",
       "\n",
       "                              S.F.Ratio  perc.alumni  Expend  Grad.Rate  \n",
       "Abilene Christian University       18.1           12    7041         60  \n",
       "Adelphi University                 12.2           16   10527         56  \n",
       "Adrian College                     12.9           30    8735         54  \n",
       "Agnes Scott College                 7.7           37   19016         59  \n",
       "Alaska Pacific University          11.9            2   10922         15  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1. a) Using the pandas library to read the csv data file and create a data-frame called college\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Defining the bucket\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'data-445-bucket-callaghan'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "## Defining the csv file\n",
    "file_key = 'College.csv'\n",
    "\n",
    "bucket_object = bucket.Object(file_key)\n",
    "file_object = bucket_object.get()\n",
    "file_content_stream = file_object.get('Body')\n",
    "\n",
    "college = pd.read_csv(file_content_stream)\n",
    "\n",
    "college.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "666578fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## b) Changing the Private variable from a categorical variable to a numerical variable\n",
    "\n",
    "college['Private'] = np.where(college['Private'] == 'Yes', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7741560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## c) Using Private, F.Undergrad, P.Undergrad, Outstate, Room.Board, Books, Personal, S.F.Ratio and Grad.Rate as the predictor variables, \n",
    "## and Apps as the target variable to split the data into train (80%) and test (20%)\n",
    "\n",
    "X = college[['Private', 'F.Undergrad', 'P.Undergrad', 'Outstate', 'Room.Board', 'Books', 'Personal', 'S.F.Ratio', 'Grad.Rate']]\n",
    "Y = college['Apps']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0920d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## d) Using the MinMaxScaler to transform the input variables in the train and test dataset to 0-1 scale\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a55753d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of Linear Regression Model: 3211896.6352141965\n"
     ]
    }
   ],
   "source": [
    "## e) Using the train dataset to build a linear regression model\n",
    "\n",
    "## Building the model\n",
    "lm_md = LinearRegression().fit(X_train, Y_train)\n",
    "\n",
    "# Predicting the number of applications on the test set\n",
    "lm_preds = lm_md.predict(X_test)\n",
    "\n",
    "## Computing the MSE\n",
    "lm_mse = np.mean(np.power(lm_preds - Y_test, 2))\n",
    "\n",
    "print('MSE of Linear Regression Model:', lm_mse)\n",
    "\n",
    "## The MSE of the first linear regression model is about 3,211,896"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05bc514c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of Ridge Regression Model: 3162770.0584494807\n"
     ]
    }
   ],
   "source": [
    "## f) Using the train dataset to build a ridge regression model as follows:\n",
    "\n",
    "## Estimating the optimal lambda via cross-validation using 5-folds\n",
    "ridge_cv = RidgeCV(alphas = np.linspace(0.001, 100, num = 100), cv = 5).fit(X_train, Y_train)\n",
    "\n",
    "## Extracting the optimal lambda value\n",
    "ridge_alpha = ridge_cv.alpha_\n",
    "\n",
    "## Building the ridge model with the optimal lambda\n",
    "ridge_md = Ridge(alpha = ridge_alpha).fit(X_train, Y_train)\n",
    "\n",
    "## Predicting on the test set\n",
    "ridge_preds = ridge_md.predict(X_test)\n",
    "\n",
    "## Calculating the MSE of ridge model\n",
    "ridge_mse = np.mean(np.power(ridge_preds - Y_test, 2))\n",
    "\n",
    "print('MSE of Ridge Regression Model:', ridge_mse)\n",
    "\n",
    "## The MSE of the first linear regression model is about 3,162,770"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0299d95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of Lasso Regression Model: 3111938.8207595623\n"
     ]
    }
   ],
   "source": [
    "## g) Using the train dataset to build a LASSO regression model as follows:\n",
    "\n",
    "## Estimating the optimal lambda via cross-validation using 5-folds\n",
    "lasso_cv = LassoCV(alphas = np.linspace(0.001, 100, num = 100), cv = 5).fit(X_train, Y_train)\n",
    "\n",
    "## Extracting the optimal alpha \n",
    "lasso_alpha = lasso_cv.alpha_\n",
    "\n",
    "## Using the optimal lambda to build the LASSO regression model\n",
    "lasso_md = Lasso(alpha = lasso_alpha).fit(X_train, Y_train)\n",
    "\n",
    "## Predicting on the test set\n",
    "lasso_preds = lasso_md.predict(X_test)\n",
    "\n",
    "## Calculating the MSE of lasso model\n",
    "lasso_mse = np.mean(np.power(lasso_preds - Y_test, 2))\n",
    "\n",
    "print('MSE of Lasso Regression Model:', lasso_mse)\n",
    "\n",
    "## The MSE of the first linear regression model is about 3,111,938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d54277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## h) Using the results from parts (e), (f) and (g), I would use the Lasso Regression model to predict the number \n",
    "## of applications that a university will receive because it has the smallest MSE value of the three considered\n",
    "## models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69596805",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. a) Using the pandas library to read the csv data file and create two data-frames called: \n",
    "## telecom train (for churn-bigml-80.csv) and telecom test (for churn-bigml-20.csv)\n",
    "\n",
    "## Defining the csv file\n",
    "file_key = 'churn-bigml-80.csv'\n",
    "file_key_2 = 'churn-bigml-20.csv'\n",
    "\n",
    "bucket_object = bucket.Object(file_key)\n",
    "bucket_object_2 = bucket.Object(file_key_2)\n",
    "\n",
    "file_object = bucket_object.get()\n",
    "file_object_2 = bucket_object_2.get()\n",
    "\n",
    "file_content_stream = file_object.get('Body')\n",
    "file_content_stream_2 = file_object_2.get('Body')\n",
    "\n",
    "telecom_train = pd.read_csv(file_content_stream)\n",
    "telecom_test = pd.read_csv(file_content_stream_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd801b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## b) Variable engineering:\n",
    "\n",
    "## Changing the Churn variable from a categorical variable to a numerical variable\n",
    "telecom_train['Churn'] = np.where(telecom_train['Churn'] == False, 0, 1)\n",
    "telecom_test['Churn'] = np.where(telecom_test['Churn'] == False, 0, 1)\n",
    "\n",
    "## Changing the International plan variable from a categorical variable to a numerical variable\n",
    "telecom_train['International_plan'] = np.where(telecom_train['International_plan'] == 'No', 0, 1)\n",
    "telecom_test['International_plan'] = np.where(telecom_test['International_plan'] == 'No', 0, 1)\n",
    "\n",
    "\n",
    "## Changing the Voice mail plan variable from a categorical variable to a numerical variable\n",
    "telecom_train['Voice_mail_plan'] = np.where(telecom_train['Voice_mail_plan'] == 'No', 0, 1)\n",
    "telecom_test['Voice_mail_plan'] = np.where(telecom_test['Voice_mail_plan'] == 'No', 0, 1)\n",
    "\n",
    "\n",
    "## Creating a new variable called 'total_charge' as the sum of 'Total_day_charge', 'Total_eve_charge', 'Total_night_charge',\n",
    "## and 'Total_intl_charge' in both dataframes\n",
    "telecom_train['total_charge'] = telecom_train['Total_day_charge'] + telecom_train['Total_eve_charge'] + telecom_train['Total_night_charge'] + telecom_train['Total_intl_charge']\n",
    "telecom_test['total_charge'] = telecom_test['Total_day_charge'] + telecom_test['Total_eve_charge'] + telecom_test['Total_night_charge'] + telecom_test['Total_intl_charge']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b71086",
   "metadata": {},
   "outputs": [],
   "source": [
    "## c) In both data-frames telecom train and telecom test, only keeping the following variables: Account length, International plan, \n",
    "## Voice mail plan, total charge, Customer service calls, and Churn.\n",
    "\n",
    "telecom_train = telecom_train[['Account_length', 'International_plan', 'Voice_mail_plan', 'total_charge', 'Customer_service_calls', 'Churn']]\n",
    "telecom_test = telecom_test[['Account_length', 'International_plan', 'Voice_mail_plan', 'total_charge', 'Customer_service_calls', 'Churn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f994027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## d) Considering the telecom train dataset and using Account length, International plan, Voice mail plan, total charge, and \n",
    "## Customer service calls as the input variables, and Churn is the target variable to do the following:\n",
    "\n",
    "X = telecom_train[['Account_length', 'International_plan', 'Voice_mail_plan', 'total_charge', 'Customer_service_calls']]\n",
    "Y = telecom_train['Churn']\n",
    "\n",
    "## Repeating steps 1000 times and storing the estimated model coefficients of each iteration in a data-frame\n",
    "\n",
    "## Defining coefficients data frame\n",
    "coefficients = pd.DataFrame(columns = ['Account_length', 'International_plan', 'Voice_mail_plan', 'total_charge', 'Customer_service_calls'])\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    \n",
    "    ## Splitting the data into train (80%) and test (20%) (taking into account the proportion of 0s and 1s in the data) \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, stratify = Y)\n",
    "\n",
    "    ## Using MinMaxScaler to transform all the input variables in the train and test datasets to 0-1 scale\n",
    "    scaler = MinMaxScaler(feature_range = (0,1))\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "    ## Estimating the optimal lambda for the LASSO model using default values for lambda in scikit-learn and 5-folds\n",
    "    lasso_cv = LassoCV(cv = 5).fit(X_train, Y_train)\n",
    "    lasso_alpha = lasso_cv.alpha_\n",
    "\n",
    "    ## Performing LASSO as a variable selector\n",
    "    lasso_md = Lasso(alpha = lasso_alpha).fit(X_train, Y_train)\n",
    "    \n",
    "    ## Extracting the estimated coefficients and putting them into a data frame\n",
    "    coefs = pd.DataFrame(lasso_md.coef_).T\n",
    "    coefs.columns = ['Account_length', 'International_plan', 'Voice_mail_plan', 'total_charge', 'Customer_service_calls']\n",
    "    \n",
    "    ## Appending the coefficients into the data frame for all coefficients\n",
    "    dataframes = [coefficients, coefs]\n",
    "    coefficients = pd.concat(dataframes)\n",
    "\n",
    "    \n",
    "coefficients = coefficients.reset_index().drop(columns = ['index'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c77cfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Number: 0\n",
      "Zero Count: 289\n",
      "Column Number: 1\n",
      "Zero Count: 0\n",
      "Column Number: 2\n",
      "Zero Count: 0\n",
      "Column Number: 3\n",
      "Zero Count: 0\n",
      "Column Number: 4\n",
      "Zero Count: 0\n"
     ]
    }
   ],
   "source": [
    "## Removing the variables whose estimated coefficients is 0 more than 200 times\n",
    "\n",
    "n = coefficients.shape[0]\n",
    "m = coefficients.shape[1]\n",
    "\n",
    "for i in range(0, m):\n",
    "    \n",
    "    zero_count = 0\n",
    "    \n",
    "    for j in range(0, n):\n",
    "        \n",
    "        if (coefficients.iloc[j, i] == 0):\n",
    "            zero_count += 1\n",
    "            \n",
    "    print(\"Column Number:\", i)\n",
    "    print(\"Zero Count:\", zero_count)\n",
    "    \n",
    "## Since the column \"Account_length\" has been set to zero more than 200 times, we are dropping it from the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb756ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_train = telecom_train.drop(columns = ['Account_length'], axis = 1)\n",
    "telecom_test = telecom_test.drop(columns = ['Account_length'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbcac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## e) Using Churn as the target variable, and the remaining variables as the input variables to do the following:\n",
    "\n",
    "X = telecom_train[['International_plan', 'Voice_mail_plan', 'total_charge', 'Customer_service_calls']]\n",
    "Y = telecom_train['Churn']\n",
    "\n",
    "## Using MinMaxScaler to transform all the input variables in the train and test datasets to 0-1 scale\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "## Defining empty lists to store results\n",
    "md1_results = []\n",
    "md2_results = []\n",
    "md3_results = []\n",
    "md4_results = []\n",
    "\n",
    "## Splitting the data into 5-folds taking into account the proportion of 0s and 1s in the data\n",
    "kfold = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_ix, test_ix in kfold.split(X, Y):\n",
    "    \n",
    "    ## Defining the data \n",
    "    X_train, X_val = X.loc[train_ix], X.loc[test_ix]\n",
    "    Y_train, Y_val = Y.loc[train_ix], Y.loc[test_ix]\n",
    "    \n",
    "    \n",
    "    #################\n",
    "    ## FIRST MODEL ##\n",
    "    #################\n",
    "    \n",
    "    ## Building a multi-layer perceptron (MLP) model with one single hidden layer with 5 neurons (hyperbolic tangent as \n",
    "    ## the activation function) and softmax as the activation function for the output. Using the stochastic descent gradient \n",
    "    ## as the method to estimate the weights (optimizer = ’sgd’, loss = ’categorical crossentropy’, and metrics = [’accuracy’]).\n",
    "    ## Use epochs = 100 and batch size = 100 to build the model\n",
    "    \n",
    "    md1 = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(5, input_dim = 4, activation = 'tanh'),\n",
    "        tf.keras.layers.Dense(2, activation = 'softmax')\n",
    "    ])\n",
    "\n",
    "    md1.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    ## Fitting the model and using the model to predict on the test dataset\n",
    "    model1 = md1.fit(X_train, tf.keras.utils.to_categorical(Y_train, num_classes = 2), epochs = 100, batch_size = 100,\n",
    "               validation_data = (X_val, tf.keras.utils.to_categorical(Y_val, num_classes = 2)), verbose = 0)\n",
    "\n",
    "    ## Reporting the recall score of this model. Using 10% as the cut-off value\n",
    "    md1_preds = md1.predict(X_val)[:, 1]\n",
    "    md1_preds = np.where(md1_preds < 0.1, 0, 1)\n",
    "    \n",
    "    md1_results.append(recall_score(Y_val, md1_preds))\n",
    "    \n",
    "    \n",
    "    ##################\n",
    "    ## SECOND MODEL ##\n",
    "    ##################\n",
    "    \n",
    "    ## Building a multi-layer perceptron (MLP) model with one single hidden layer with 5 neurons (ReLU as the \n",
    "    ## activation function) and softmax as the activation function for the output. Using the stochastic descent \n",
    "    ## gradient as the method to estimate the weights (optimizer = ’sgd’, loss = ’categorical crossentropy’ and metrics \n",
    "    ## = [’accuracy’]). Using epochs = 100 and batch size = 100 to build the model\n",
    "    \n",
    "    md2 = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(5, input_dim = 4, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(2, activation = 'softmax')\n",
    "    ])\n",
    "\n",
    "    md2.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    ## Fitting the model and using the model to predict on the test dataset\n",
    "    model2 = md2.fit(X_train, tf.keras.utils.to_categorical(Y_train, num_classes = 2), epochs = 100, batch_size = 100,\n",
    "               validation_data = (X_val, tf.keras.utils.to_categorical(Y_val, num_classes = 2)), verbose = 0)\n",
    "\n",
    "    ## Reporting the recall score of this model. Using 10% as the cut-off value\n",
    "    md2_preds = md2.predict(X_val)[:, 1]\n",
    "    md2_preds = np.where(md2_preds < 0.1, 0, 1)\n",
    "    \n",
    "    md2_results.append(recall_score(Y_val, md2_preds))\n",
    "    \n",
    "    \n",
    "    #################\n",
    "    ## THIRD MODEL ##\n",
    "    #################\n",
    "    \n",
    "    ## Building a support vector machine model using rbf as the kernel. After that, using the model to predict on the test dataset \n",
    "    md3 = SVC(kernel = 'rbf', probability = True).fit(X_train, Y_train)\n",
    "\n",
    "    ## Predicting on the test data set. Using 10% as the cut-off value.\n",
    "    md3_preds = md3.predict_proba(X_val)[:, 1]\n",
    "    md3_preds = np.where(md3_preds < 0.1, 0, 1)\n",
    "\n",
    "    md3_results.append(recall_score(Y_val, md3_preds))\n",
    "    \n",
    "    \n",
    "    ##################\n",
    "    ## FOURTH MODEL ##\n",
    "    ##################\n",
    "    \n",
    "    ## Building a support vector machine model using poly as the kernel. After that, using the model to predict on the test dataset\n",
    "    md4 = SVC(kernel = 'poly', probability = True).fit(X_train, Y_train)\n",
    "\n",
    "    ## Predicting on the test data set. Using 10% as the cut-off value.\n",
    "    md4_preds = md4.predict_proba(X_val)[:, 1]\n",
    "    md4_preds = np.where(md4_preds < 0.1, 0, 1)\n",
    "\n",
    "    md4_results.append(recall_score(Y_val, md4_preds))\n",
    "    \n",
    "\n",
    "    \n",
    "print('Average Recall Score for Model 1:', np.mean(md1_results))\n",
    "print('Average Recall Score for Model 2:', np.mean(md2_results))\n",
    "print('Average Recall Score for Model 3:', np.mean(md3_results))\n",
    "print('Average Recall Score for Model 4:', np.mean(md4_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45525ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## f) Repeating part (e) 100 times\n",
    "\n",
    "X = telecom_train[['International_plan', 'Voice_mail_plan', 'total_charge', 'Customer_service_calls']]\n",
    "Y = telecom_train['Churn']\n",
    "\n",
    "## Using MinMaxScaler to transform all the input variables in the train and test datasets to 0-1 scale\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "## Defining empty lists to store kfold iteration average recall scores\n",
    "md1_100 = []\n",
    "md2_100 = []\n",
    "md3_100 = []\n",
    "md4_100 = []\n",
    "\n",
    "\n",
    "for i in range(0, 100):\n",
    "    \n",
    "    ## Defining empty lists to store recall scores\n",
    "    md1_results = []\n",
    "    md2_results = []\n",
    "    md3_results = []\n",
    "    md4_results = []\n",
    "\n",
    "    ## Splitting the data into 5-folds taking into account the proportion of 0s and 1s in the data\n",
    "    kfold = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "    for train_ix, test_ix in kfold.split(X, Y):\n",
    "    \n",
    "        ## Defining the data \n",
    "        X_train, X_val = X.loc[train_ix], X.loc[test_ix]\n",
    "        Y_train, Y_val = Y.loc[train_ix], Y.loc[test_ix]\n",
    "    \n",
    "    \n",
    "        #################\n",
    "        ## FIRST MODEL ##\n",
    "        #################\n",
    "    \n",
    "        ## Building a multi-layer perceptron (MLP) model with one single hidden layer with 5 neurons (hyperbolic tangent as \n",
    "        ## the activation function) and softmax as the activation function for the output. Using the stochastic descent gradient \n",
    "        ## as the method to estimate the weights (optimizer = ’sgd’, loss = ’categorical crossentropy’, and metrics = [’accuracy’]).\n",
    "        ## Use epochs = 100 and batch size = 100 to build the model\n",
    "    \n",
    "        md1 = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(5, input_dim = 4, activation = 'tanh'),\n",
    "            tf.keras.layers.Dense(2, activation = 'softmax')\n",
    "        ])\n",
    "\n",
    "        md1.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "        ## Fitting the model and using the model to predict on the test dataset\n",
    "        model1 = md1.fit(X_train, tf.keras.utils.to_categorical(Y_train, num_classes = 2), epochs = 100, batch_size = 100,\n",
    "                   validation_data = (X_val, tf.keras.utils.to_categorical(Y_val, num_classes = 2)), verbose = 0)\n",
    "\n",
    "        ## Reporting the recall score of this model. Using 10% as the cut-off value\n",
    "        md1_preds = md1.predict(X_val)[:, 1]\n",
    "        md1_preds = np.where(md1_preds < 0.1, 0, 1)\n",
    "    \n",
    "        md1_results.append(recall_score(Y_val, md1_preds))\n",
    "    \n",
    "    \n",
    "        ##################\n",
    "        ## SECOND MODEL ##\n",
    "        ##################\n",
    "    \n",
    "        ## Building a multi-layer perceptron (MLP) model with one single hidden layer with 5 neurons (ReLU as the \n",
    "        ## activation function) and softmax as the activation function for the output. Using the stochastic descent \n",
    "        ## gradient as the method to estimate the weights (optimizer = ’sgd’, loss = ’categorical crossentropy’ and metrics \n",
    "        ## = [’accuracy’]). Using epochs = 100 and batch size = 100 to build the model\n",
    "    \n",
    "        md2 = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(5, input_dim = 4, activation = 'relu'),\n",
    "            tf.keras.layers.Dense(2, activation = 'softmax')\n",
    "        ])\n",
    "\n",
    "        md2.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "        ## Fitting the model and using the model to predict on the test dataset\n",
    "        model2 = md2.fit(X_train, tf.keras.utils.to_categorical(Y_train, num_classes = 2), epochs = 100, batch_size = 100,\n",
    "                   validation_data = (X_val, tf.keras.utils.to_categorical(Y_val, num_classes = 2)), verbose = 0)\n",
    "\n",
    "        ## Reporting the recall score of this model. Using 10% as the cut-off value\n",
    "        md2_preds = md2.predict(X_val)[:, 1]\n",
    "        md2_preds = np.where(md2_preds < 0.1, 0, 1)\n",
    "    \n",
    "        md2_results.append(recall_score(Y_val, md2_preds))\n",
    "    \n",
    "    \n",
    "        #################\n",
    "        ## THIRD MODEL ##\n",
    "        #################\n",
    "    \n",
    "        ## Building a support vector machine model using rbf as the kernel. After that, using the model to predict on the test dataset \n",
    "        md3 = SVC(kernel = 'rbf', probability = True).fit(X_train, Y_train)\n",
    "\n",
    "        ## Predicting on the test data set. Using 10% as the cut-off value.\n",
    "        md3_preds = md3.predict_proba(X_val)[:, 1]\n",
    "        md3_preds = np.where(md3_preds < 0.1, 0, 1)\n",
    "\n",
    "        md3_results.append(recall_score(Y_val, md3_preds))\n",
    "    \n",
    "    \n",
    "        ##################\n",
    "        ## FOURTH MODEL ##\n",
    "        ##################\n",
    "    \n",
    "        ## Building a support vector machine model using poly as the kernel. After that, using the model to predict on the test dataset\n",
    "        md4 = SVC(kernel = 'poly', probability = True).fit(X_train, Y_train)\n",
    "\n",
    "        ## Predicting on the test data set. Using 10% as the cut-off value.\n",
    "        md4_preds = md4.predict_proba(X_val)[:, 1]\n",
    "        md4_preds = np.where(md4_preds < 0.1, 0, 1)\n",
    "\n",
    "        md4_results.append(recall_score(Y_val, md4_preds))\n",
    "        \n",
    "        \n",
    "        ## Appending mean recall score \n",
    "        md1_100.append(np.mean(md1_results))\n",
    "        md2_100.append(np.mean(md2_results))\n",
    "        md3_100.append(np.mean(md3_results))\n",
    "        md4_100.append(np.mean(md4_results))\n",
    "    \n",
    "\n",
    "    \n",
    "## Reporting the average recall of each of the models for the 100 repetitions\n",
    "print('Over the 100 iterations...\\n')\n",
    "print('Average Recall Score for Model 1:', np.mean(md1_100))\n",
    "print('Average Recall Score for Model 2:', np.mean(md2_100))\n",
    "print('Average Recall Score for Model 3:', np.mean(md3_100))\n",
    "print('Average Recall Score for Model 4:', np.mean(md4_100))\n",
    "\n",
    "\n",
    "## Creating a visualization that shows the recall value for each of the models at each iteration\n",
    "fig = plt.figure(figsize = (18, 12))\n",
    "iterations = range(0, 100)\n",
    "\n",
    "plt.plot(iterations, md1_100, marker = 'o', color = 'orangered', label = 'Model 1')\n",
    "plt.plot(iterations, md2_100, marker = 'o', color = 'dodgerblue', label = 'Model 2')\n",
    "plt.plot(iterations, md3_100, marker = 'o', color = 'mediumorchid', label = 'Model 3')\n",
    "plt.plot(iterations, md4_100, marker = 'o', color = 'indigo', label = 'Model 4')\n",
    "plt.title('Model Average Recall Scores by Iteration')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Recall Score')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127a8272",
   "metadata": {},
   "outputs": [],
   "source": [
    "## g)\n",
    "\n",
    "## Defining the input/target variables for the train and test sets\n",
    "X_train = telecom_train[['International_plan', 'Voice_mail_plan', 'total_charge', 'Customer_service_calls']]\n",
    "X_test = telecom_test[['International_plan', 'Voice_mail_plan', 'total_charge', 'Customer_service_calls']]\n",
    "\n",
    "Y_train = telecom_train['Churn']\n",
    "Y_test = telecom_test['Churn']\n",
    "\n",
    "\n",
    "## Using the MinMaxScaler function to transform each of the input variables in the telecom train and telecom test data-frames to a 0-1 scale\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "## Using telecom_train to build two models: the better MLP model (Model 1) and the better support vector machine (Model 3)\n",
    "\n",
    "###############\n",
    "## MLP Model ##\n",
    "###############\n",
    "\n",
    "mlp = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(5, input_dim = 4, activation = 'tanh'),\n",
    "    tf.keras.layers.Dense(2, activation = 'softmax')\n",
    "])\n",
    "\n",
    "mlp.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "## Fitting the model and using the model to predict on the test dataset\n",
    "model1 = mlp.fit(X_train, tf.keras.utils.to_categorical(Y_train, num_classes = 2), epochs = 100, batch_size = 100,\n",
    "                 validation_data = (X_test, tf.keras.utils.to_categorical(Y_test, num_classes = 2)), verbose = 0)\n",
    "\n",
    "## Predicting the likelihood of Churn on the telecom_test data-frame (Using 10% as the cut-off value)\n",
    "mlp_preds = mlp.predict(X_test)[:, 1]\n",
    "mlp_preds = np.where(mlp_preds < 0.1, 0, 1)\n",
    "    \n",
    "## Computing the recall of the MLP Model\n",
    "mlp_recall = recall_score(Y_test, mlp_preds)\n",
    "\n",
    "print('Recall Score of MLP Model:', mlp_recall)\n",
    "\n",
    "\n",
    "###############\n",
    "## SVM Model ##\n",
    "###############\n",
    "\n",
    "svm = SVC(kernel = 'rbf', probability = True).fit(X_train, Y_train)\n",
    "\n",
    "## Predicting the likelihood of Churn on the telecom_test data-frame (Using 10% as the cut-off value)\n",
    "svm_preds = svm.predict_proba(X_test)[:, 1]\n",
    "svm_preds = np.where(svm_preds < 0.1, 0, 1)\n",
    "\n",
    "## Computing the recall of the MLP Model\n",
    "svm_recall = recall_score(Y_test, svm_preds)\n",
    "\n",
    "print('Recall Score of SVM Model:', svm_recall)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
