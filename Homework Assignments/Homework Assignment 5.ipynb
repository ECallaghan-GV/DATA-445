{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d430a8d",
   "metadata": {},
   "source": [
    "# Homework Assignment 5 - Evan Callaghan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d3d8d4",
   "metadata": {},
   "source": [
    "### 1. If a decision tree is under-fitting the training dataset, is it a good idea to try scaling the input features?\n",
    "\n",
    "#### If a decision tree is under-fitting the training dataset, it will not make a difference if the input features are scaled or not. Decision trees do not require feature scaling to be performed in order for them to be used in the model.\n",
    "\n",
    "### 2.  If a decision tree is over-fitting the training dataset, is it a good idea to try decreasing max depth?\n",
    "\n",
    "#### A large max depth value can lead to excessive complexity in a model, leading to over-fitting the data. Therefore, if a decision tree is over-fitting the training dataset, it is a good idea to decrease the max depth as that will create a more simplistic model.\n",
    "\n",
    "### 3. Why would you use a random forest instead of a decision tree?\n",
    "\n",
    "#### D. You would use a random forest instead of a decision tree for a lower training error and to reduce the variance of the model.\n",
    "\n",
    "### 4. Which of the following is/are TRUE about bagging trees?\n",
    "\n",
    "#### D. In bagging trees, the trees are grown independent of each other. And bagging is a method for improving the performance by aggregating the results of weak learners.\n",
    "\n",
    "### 5.  Suppose you are building random forest model which splits a node on the attribute that has highest information gain (using the Gini index). In the below image, which attribute has the highest information gain? Show all your calculations.\n",
    "\n",
    "#### Outlook Node:\n",
    "#### Gini_leaf1 = 1 - (2/5)^2 - (3/5)^2 = 12/25\n",
    "#### Gini_leaf2 = 1 - (4/4)^2 - (0/4)^2 = 0\n",
    "#### Gini_leaf3 = 1 - (3/5)^2 - (2/5)^2 = 12/25\n",
    "#### Gini Index = 0.32\n",
    "\n",
    "#### Temperature Node:\n",
    "#### Gini_leaf1 = 1 - (2/4)^2 - (2/4)^2 = 0.5\n",
    "#### Gini_leaf2 = 1 - (4/6)^2 - (2/6)^2 = 0.444\n",
    "#### Gini_leaf3 = 1 - (3/4)^2 - (1/4)^2 = 0.375\n",
    "#### Gini Index = 0.4398\n",
    "\n",
    "#### Windy Node:\n",
    "#### Gini_leaf1 = 1 - (6/8)^2 - (2/8)^2 = 0.375\n",
    "#### Gini_leaf2 = 1 - (3/6)^2 - (3/6)^2 = 0.5\n",
    "#### Gini Index = 0.4375\n",
    "\n",
    "#### Humidity Node:\n",
    "#### Gini_leaf1 = 1 - (3/7)^2 - (4/7)^2 = 0.4898\n",
    "#### Gini_leaf2 = 1 - (6/7)^2 - (1/7)^2 = 0.2449\n",
    "#### Gini Index = 0.3673\n",
    "\n",
    "#### Result: The Outlook attribute has the highest information gain because it has the lowest average Gini index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6560311c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   male  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n",
       "0     1   39        4.0              0         0.0     0.0                0   \n",
       "1     0   46        2.0              0         0.0     0.0                0   \n",
       "2     1   48        1.0              1        20.0     0.0                0   \n",
       "3     0   61        3.0              1        30.0     0.0                0   \n",
       "4     0   46        3.0              1        23.0     0.0                0   \n",
       "\n",
       "   prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  glucose  \\\n",
       "0             0         0    195.0  106.0   70.0  26.97       80.0     77.0   \n",
       "1             0         0    250.0  121.0   81.0  28.73       95.0     76.0   \n",
       "2             0         0    245.0  127.5   80.0  25.34       75.0     70.0   \n",
       "3             1         0    225.0  150.0   95.0  28.58       65.0    103.0   \n",
       "4             0         0    285.0  130.0   84.0  23.10       85.0     85.0   \n",
       "\n",
       "   TenYearCHD  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           1  \n",
       "4           0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 6. a) Using the pandas library to read the csv data file and create a data-frame called heart\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Defining the bucket\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'data-445-bucket-callaghan'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "## Defining the csv file\n",
    "file_key = 'framingham.csv'\n",
    "\n",
    "bucket_object = bucket.Object(file_key)\n",
    "file_object = bucket_object.get()\n",
    "file_content_stream = file_object.get('Body')\n",
    "\n",
    "heart = pd.read_csv(file_content_stream)\n",
    "\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77429a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "## b)  Removing observations with missing values\n",
    "\n",
    "heart = heart.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac428249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male               0.021398\n",
       "age                0.125094\n",
       "education          0.036859\n",
       "currentSmoker      0.012567\n",
       "cigsPerDay         0.050446\n",
       "BPMeds             0.007056\n",
       "prevalentStroke    0.003277\n",
       "prevalentHyp       0.018345\n",
       "diabetes           0.006627\n",
       "totChol            0.122062\n",
       "sysBP              0.134741\n",
       "diaBP              0.118751\n",
       "BMI                0.127023\n",
       "heartRate          0.096058\n",
       "glucose            0.119696\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## c)  Using all the available variables as the predictor variables, and TenYearCHD as the target variable, to do the following:\n",
    "\n",
    "## (i) Splitting the data into train (80%) and test (20%) (taking into account the proportion of 0s and 1s)\n",
    "## (ii) Using the train data-frame to build a random forest classifier (using 500 trees).\n",
    "### (iii) Extract the feature importance of each of the variables.\n",
    "\n",
    "## Defining the empty data frame to store results\n",
    "importance = pd.DataFrame(columns = heart.columns)\n",
    "importance = importance.drop(columns = ['TenYearCHD'])\n",
    "\n",
    "## Repeating (i)-(iii) 100 times\n",
    "for i in range(0, 100):\n",
    "\n",
    "    ## Defining the input and target variables\n",
    "    X = heart.drop(columns = ['TenYearCHD'])\n",
    "    Y = heart['TenYearCHD']\n",
    "\n",
    "    ## Splitting the data\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify = Y)\n",
    "    \n",
    "    ## Building the model\n",
    "    rf_md = RandomForestClassifier(n_estimators = 500).fit(X_train, Y_train)\n",
    "\n",
    "    ## Extracting the feature importance\n",
    "    importance.loc[i] = rf_md.feature_importances_.T\n",
    "\n",
    "    \n",
    "## Computing the average importance of each of the variables across the 100 splits.\n",
    "importance.mean()\n",
    "\n",
    "\n",
    "## sysBP, BMI, age, totChol, and glucose are found to be the five most important variables on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a5fdd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Recall Score Model 1: 0.8360714285714286\n",
      "Average Recall Score Model 2: 0.8218750000000001\n",
      "Average Recall Score Model 3: 0.8080357142857143\n"
     ]
    }
   ],
   "source": [
    "## d) Using the top 5 variables from part (c) as the predictor variables and TenYearCHD as the target variable to do the following:\n",
    "\n",
    "## (i) Splitting the data into train (80%) and test (20%) (taking into account the proportion of 0s and 1s)\n",
    "## (ii) Using the train data-frame to build a random forest classifier (using 500 trees and maximum depth tree equal to 3). Using this \n",
    "## model to predict on the test set\n",
    "## (iii) Using the train data-frame to build a random forest classifier (using 500 trees and maximum depth tree equal to 5). Using this \n",
    "## model to predict on the test set\n",
    "## (iv) Using the train data-frame to build a random forest classifier (using 500 trees and maximum depth tree equal to 7). Using this \n",
    "## model to predict on the test set\n",
    "\n",
    "## Initalizing empty lists to store results\n",
    "md1_recall = []\n",
    "md2_recall = []\n",
    "md3_recall = []\n",
    "\n",
    "## Repeating (i)-(iv) 100 times\n",
    "for i in range(0, 100):\n",
    "\n",
    "    ## Defining the input and target variables\n",
    "    X = heart[['sysBP', 'BMI', 'age', 'totChol', 'glucose']]\n",
    "    Y = heart['TenYearCHD']\n",
    "\n",
    "    ## Splitting the data\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify = Y)\n",
    "    \n",
    "    \n",
    "    ## Model 1\n",
    "    ## --------------------------------------------------------\n",
    "\n",
    "    ## Building the model\n",
    "    md1 = RandomForestClassifier(n_estimators = 500, max_depth = 3).fit(X_train, Y_train)\n",
    "    \n",
    "    ## Predicting on the test set\n",
    "    md1_preds = md1.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    ## Using 10% as cutoff value and reporting the recall\n",
    "    md1_preds = np.where(md1_preds < 0.1, 0, 1)\n",
    "    \n",
    "    md1_recall.append(recall_score(Y_test, md1_preds))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Model 2\n",
    "    ## --------------------------------------------------------\n",
    "\n",
    "    ## Building the model\n",
    "    md2 = RandomForestClassifier(n_estimators = 500, max_depth = 5).fit(X_train, Y_train)\n",
    "    \n",
    "    ## Predicting on the test set\n",
    "    md2_preds = md2.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    ## Using 10% as cutoff value and reporting the recall\n",
    "    md2_preds = np.where(md2_preds < 0.1, 0, 1)\n",
    "    \n",
    "    md2_recall.append(recall_score(Y_test, md2_preds))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Model 3\n",
    "    ## --------------------------------------------------------\n",
    "\n",
    "    ## Building the model\n",
    "    md3 = RandomForestClassifier(n_estimators = 500, max_depth = 7).fit(X_train, Y_train)\n",
    "    \n",
    "    ## Predicting on the test set\n",
    "    md3_preds = md3.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    ## Using 10% as cutoff value and reporting the recall\n",
    "    md3_preds = np.where(md3_preds < 0.1, 0, 1)\n",
    "    \n",
    "    md3_recall.append(recall_score(Y_test, md3_preds))\n",
    "    \n",
    "\n",
    "## Computing the average recall of each of the models across the 100 iterations\n",
    "print('Average Recall Score Model 1:', np.mean(md1_recall))\n",
    "print('Average Recall Score Model 2:', np.mean(md2_recall))\n",
    "print('Average Recall Score Model 3:', np.mean(md3_recall))\n",
    "\n",
    "\n",
    "## We would use Model 1 (max_depth = 3) to predict TenYearCHD becasue it has the highest recall score of the \n",
    "## three considered models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
