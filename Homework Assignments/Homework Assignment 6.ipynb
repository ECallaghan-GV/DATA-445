{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9470203",
   "metadata": {},
   "source": [
    "# Homework Assignment 6 - Evan Callaghan\n",
    "\n",
    "### 1. A data scientist is running an AdaBoost classifier on a dataset with 100 observations. Answer the following:\n",
    "\n",
    "### a) What is the weight initial weight of observation 72th in the training dataset?\n",
    "\n",
    "#### Initially, an AdaBoost classifier weighs all observations equally. Since there is 100 observations under consideration, the initial weight of the 72nd observation is 1/100 (0.01). \n",
    "\n",
    "### b) The 72nd observation in the training dataset is misclassified by the first weak learner chosen by the data scientist. Is the new weight of the 72nd observation in the training dataset larger or smaller than the weight assigned to that observation initially?\n",
    "\n",
    "#### AdaBoost works to increase the weight of misclassified observations in order to force the learner to focus on the regions that were more problematic for the previous estimators. Therefore, since the 72nd observation is misclassified by the first learner, the new weight will be larger than the weight initially assigned. \n",
    "\n",
    "### 2. Explain why AbaBoost.M1 is an ensemble learning algorithm?\n",
    "\n",
    "#### An ensemble learning algorithm is defined as a set of weak learners that are trained together (or in a sequence) to make up a committee. In classification and regression problems, the final result is obtained by averaging the predictions or employing a majority vote. In AdaBoost.M1, a set number of weak models are added sequentially and trained using the weighted training data. Therefore, AdaBoost.M1 is an ensemble learning algorithm.\n",
    "\n",
    "\n",
    "### 3. Suppose you are running AdaBoost.M1 (with Î· = 0.1) with 4 training examples. At the start of the current iteration, the four examples have the weights shown in the following table. Another column says if the weak classifier got them correct or incorrect. Determining the new weights for these four examples:\n",
    "\n",
    "#### \n",
    "\n",
    "### 4.  If your AdaBoost ensemble under-fits the training dataset, what would you do to fix that? That is, which hyper-parameters should you tweak?\n",
    "\n",
    "#### If your AdaBoost ensemble under-fits the training dataset, I would try to increase the number of estimators or reduce the regularization hyper-parameter of the base estimator. These options could increase the complexity of the model and avoid the problem of under-fitting.\n",
    "\n",
    "\n",
    "### 5. For binary classification, which of the following statements are TRUE of AdaBoost with decision trees as learners?\n",
    "\n",
    "#### A. It usually has lower bias than a single decision tree.\n",
    "#### C. It assigns higher weights to observations that have been misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cde367",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. a) Using the pandas library to read the csv data file and create a data-frame called heart\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "## Defining the bucket\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'data-445-bucket-callaghan'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "## Defining the csv file\n",
    "file_key = 'framingham.csv'\n",
    "\n",
    "bucket_object = bucket.Object(file_key)\n",
    "file_object = bucket_object.get()\n",
    "file_content_stream = file_object.get('Body')\n",
    "\n",
    "heart = pd.read_csv(file_content_stream)\n",
    "\n",
    "## Removing observation with missing values\n",
    "heart = heart.dropna()\n",
    "\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33662899",
   "metadata": {},
   "outputs": [],
   "source": [
    "## b) Using age, totChol, sysBP,BMI, heartRate, and glucose as the predictor variables and TenYearCHD as the target \n",
    "## variable to do the following:\n",
    "\n",
    "## Defining the input and target variables:\n",
    "X = heart[['age', 'totChol', 'sysBP', 'BMI', 'heartRate', 'glucose']]\n",
    "Y = heart['TenYearCHD']\n",
    "\n",
    "## Splitting the data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify = Y)\n",
    "\n",
    "## Normalizing the input variables\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "## Defining empty lists to store results\n",
    "md1_recall = []\n",
    "md2_recall = []\n",
    "md3_recall = []\n",
    "\n",
    "\n",
    "## Repeating the steps 100 times\n",
    "for i in range(0, 100):\n",
    "\n",
    "    ## Model 1\n",
    "    ## Building a random forest classifier (using 500 trees and maximum depth tree equal to 3)\n",
    "    ## ------------------------------------------\n",
    "\n",
    "    ## Building the model\n",
    "    md1 = RandomForestClassifier(n_estimators = 500, max_depth = 3).fit(X_train, Y_train)\n",
    "\n",
    "    ## Predicting on the test set\n",
    "    md1_preds = md1.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    ## Using 10% as cutoff value and reporting the recall\n",
    "    md1_preds = np.where(md1_preds < 0.1, 0, 1)\n",
    "    md1_recall.append(recall_score(Y_test, md1_preds))\n",
    "\n",
    "\n",
    "\n",
    "    ## Model 2\n",
    "    ## Building an AdaBoost classifier (using 500 trees and maximum depth tree equal to 3)\n",
    "    ## ------------------------------------------\n",
    "\n",
    "    ## Building the model\n",
    "    md2 = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth = 3), n_estimators = 500).fit(X_train, Y_train)\n",
    "\n",
    "    ## Predicting on the test set\n",
    "    md2_preds = md2.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    ## Using 10% as cutoff value and reporting the recall\n",
    "    md2_preds = np.where(md2_preds < 0.1, 0, 1)\n",
    "    md2_recall.append(recall_score(Y_test, md2_preds))\n",
    "\n",
    "\n",
    "\n",
    "    ## Model 3\n",
    "    ## Building an AdaBoost classifier (using 50 learners and the support vector machine classifier \n",
    "    ## as the learner with rbf as the kernel)\n",
    "    ## ------------------------------------------\n",
    "\n",
    "    ## Building the model\n",
    "    md3 = AdaBoostClassifier(base_estimator = SVC(kernel = 'rbf', probability = True), n_estimators = 50).fit(X_train, Y_train)\n",
    "\n",
    "    ## Predicting on the test set\n",
    "    md3_preds = md3.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    ## Using 10% as cutoff value and reporting the recall\n",
    "    md3_preds = np.where(md3_preds < 0.1, 0, 1)\n",
    "    md3_recall.append(recall_score(Y_test, md3_preds))\n",
    "\n",
    "\n",
    "## Computing the average recall of each of the models across the 100 iterations\n",
    "print('Average Recall Score of Model 1:', np.mean(md1_recall))\n",
    "print('Average Recall Score of Model 2:', np.mean(md2_recall))\n",
    "print('Average Recall Score of Model 3:', np.mean(md3_recall))\n",
    "\n",
    "## What model would use to predict TenYearCHD? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
